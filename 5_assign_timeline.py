#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Phase 5: ä»»åŠ¡æ—¶é—´çº¿åˆ†é…
ä¸ºæ¯ä¸ªé¡¹ç›®çš„æ‰€æœ‰subtasksåˆ†é…deadlineæ—¶é—´
"""

import os
import json
import re
import time
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
from openai import OpenAI

import config
from prompt import get_task_timeline_assignment_prompt


def load_project_file(project_path: str) -> dict:
    """åŠ è½½é¡¹ç›®JSONæ–‡ä»¶"""
    with open(project_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_project_file(project_path: str, project_data: dict):
    """ä¿å­˜é¡¹ç›®JSONæ–‡ä»¶"""
    with open(project_path, 'w', encoding='utf-8') as f:
        json.dump(project_data, f, ensure_ascii=False, indent=2)


def call_gpt_for_timeline(
    project_info: dict,
    members_with_subtasks: list,
    max_retries: int = None
) -> dict:
    """
    è°ƒç”¨GPT APIè¿›è¡Œä»»åŠ¡æ—¶é—´çº¿åˆ†é…

    Returns:
        åŒ…å«task_timelineçš„å­—å…¸
    """
    if max_retries is None:
        max_retries = config.MAX_RETRIES

    client = OpenAI(
        api_key=config.OPENAI_API_KEY,
        base_url=config.OPENAI_BASE_URL
    )

    prompt = get_task_timeline_assignment_prompt(
        project_info=project_info,
        members_with_subtasks=members_with_subtasks,
        start_date=config.TIMELINE_START_DATE,
        end_date=config.TIMELINE_END_DATE
    )

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=config.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é¡¹ç›®ç®¡ç†ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                **config.API_PARAMS
            )

            content = response.choices[0].message.content.strip()

            # æ¸…ç†markdownä»£ç å—æ ‡è®°
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()

            # æ¸…ç†æ§åˆ¶å­—ç¬¦ï¼ˆç§»é™¤æœªè½¬ä¹‰çš„æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰
            content = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', content)

            # å°è¯•æå– JSONï¼ˆå¤„ç†å‰åå¯èƒ½æœ‰é¢å¤–æ–‡å­—çš„æƒ…å†µï¼‰
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
            start_idx = content.find('{')
            end_idx = content.rfind('}')

            if start_idx == -1 or end_idx == -1 or start_idx >= end_idx:
                raise ValueError(f"æ— æ³•åœ¨å“åº”ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„ JSON å¯¹è±¡\nå“åº”å‰200å­—ç¬¦: {content[:200]}")

            json_str = content[start_idx:end_idx+1]

            # è§£æJSON
            result = json.loads(json_str)

            # éªŒè¯å¿…è¦å­—æ®µ
            if 'task_timeline' not in result:
                raise ValueError("è¿”å›ç»“æœç¼ºå°‘ task_timeline å­—æ®µ")

            return result

        except json.JSONDecodeError as e:
            print(f"  âš ï¸  JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(config.RETRY_DELAY)
            else:
                raise

        except Exception as e:
            print(f"  âš ï¸  APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(config.RETRY_DELAY)
            else:
                raise


def validate_timeline(
    project_data: dict,
    timeline_result: dict,
    start_date: str,
    end_date: str
) -> dict:
    """
    éªŒè¯æ—¶é—´çº¿åˆ†é…çš„æœ‰æ•ˆæ€§

    Returns:
        éªŒè¯æŠ¥å‘Šå­—å…¸ {'valid': bool, 'errors': list, 'warnings': list}
    """
    errors = []
    warnings = []

    # 1. æ£€æŸ¥ä»»åŠ¡æ•°é‡
    total_subtasks = sum(len(m.get('subtasks', [])) for m in project_data['members'])
    assigned_tasks = len(timeline_result.get('task_timeline', []))

    if assigned_tasks != total_subtasks:
        errors.append(f"ä»»åŠ¡æ•°é‡ä¸åŒ¹é…: é¢„æœŸ{total_subtasks}ä¸ªï¼Œå®é™…åˆ†é…{assigned_tasks}ä¸ª")

    # 2. æ£€æŸ¥æ¯ä¸ªä»»åŠ¡
    task_ids_in_project = set()
    for member in project_data['members']:
        for subtask in member.get('subtasks', []):
            task_ids_in_project.add(subtask['subtask_id'])

    task_ids_assigned = set()
    for task in timeline_result.get('task_timeline', []):
        task_id = task.get('subtask_id')
        deadline = task.get('deadline')

        # æ£€æŸ¥æ˜¯å¦æœ‰subtask_id
        if task_id is None:
            errors.append(f"ä»»åŠ¡ç¼ºå°‘subtask_idå­—æ®µ")
            continue

        task_ids_assigned.add(task_id)

        # æ£€æŸ¥deadlineæ ¼å¼
        if not deadline:
            errors.append(f"ä»»åŠ¡{task_id}ç¼ºå°‘deadlineå­—æ®µ")
            continue

        try:
            deadline_date = datetime.strptime(deadline, "%Y-%m-%d")
            start = datetime.strptime(start_date, "%Y-%m-%d")
            end = datetime.strptime(end_date, "%Y-%m-%d")

            if not (start <= deadline_date <= end):
                errors.append(f"ä»»åŠ¡{task_id}çš„deadline {deadline} è¶…å‡ºèŒƒå›´ [{start_date}, {end_date}]")

        except ValueError:
            errors.append(f"ä»»åŠ¡{task_id}çš„deadlineæ ¼å¼é”™è¯¯: {deadline}")

    # 3. æ£€æŸ¥ç¼ºå¤±çš„ä»»åŠ¡
    missing_tasks = task_ids_in_project - task_ids_assigned
    if missing_tasks:
        errors.append(f"ä»¥ä¸‹ä»»åŠ¡æœªè¢«åˆ†é…æ—¶é—´: {sorted(missing_tasks)}")

    # 4. æ£€æŸ¥å¤šä½™çš„ä»»åŠ¡
    extra_tasks = task_ids_assigned - task_ids_in_project
    if extra_tasks:
        warnings.append(f"ä»¥ä¸‹ä»»åŠ¡ä¸åœ¨åŸé¡¹ç›®ä¸­: {sorted(extra_tasks)}")

    return {
        'valid': len(errors) == 0,
        'errors': errors,
        'warnings': warnings,
        'total_subtasks': total_subtasks,
        'assigned_tasks': assigned_tasks
    }


def apply_timeline_to_project(
    project_data: dict,
    timeline_result: dict
) -> dict:
    """
    å°†æ—¶é—´çº¿åˆ†é…ç»“æœåº”ç”¨åˆ°é¡¹ç›®æ•°æ®ä¸­

    Returns:
        æ›´æ–°åçš„project_data
    """
    # åˆ›å»ºsubtask_idåˆ°deadlineçš„æ˜ å°„
    deadline_map = {}
    for task in timeline_result.get('task_timeline', []):
        task_id = task.get('subtask_id')
        deadline = task.get('deadline')
        if task_id and deadline:
            deadline_map[task_id] = deadline

    # æ›´æ–°æ¯ä¸ªmemberçš„subtasks
    for member in project_data['members']:
        for subtask in member.get('subtasks', []):
            task_id = subtask['subtask_id']
            if task_id in deadline_map:
                subtask['deadline'] = deadline_map[task_id]

    return project_data


def process_single_project(
    project_path: Path,
    output_dir: Path
) -> dict:
    """
    å¤„ç†å•ä¸ªé¡¹ç›®çš„æ—¶é—´çº¿åˆ†é…

    Returns:
        å¤„ç†æŠ¥å‘Šå­—å…¸
    """
    project_name = project_path.stem
    print(f"\nå¤„ç†é¡¹ç›®: {project_name}")

    # 1. åŠ è½½é¡¹ç›®æ•°æ®
    project_data = load_project_file(str(project_path))
    # æ–°æ¶æ„ä½¿ç”¨ sub_topic_info è€Œä¸æ˜¯ project_info
    sub_topic_info = project_data.get('sub_topic_info', {})
    members = project_data.get('members', [])

    total_subtasks = sum(len(m.get('subtasks', [])) for m in members)
    print(f"  ä»»åŠ¡æ€»æ•°: {total_subtasks}")

    # 2. è°ƒç”¨GPTè¿›è¡Œæ—¶é—´çº¿åˆ†é…
    print(f"  è°ƒç”¨GPTè¿›è¡Œæ—¶é—´çº¿åˆ†é…...")
    # å°† sub_topic_info è½¬æ¢ä¸º prompt æœŸæœ›çš„æ ¼å¼
    project_info_for_prompt = {
        'project_number': sub_topic_info.get('sub_topic_id', ''),
        'project_topic': sub_topic_info.get('topic', ''),
        'project_description': sub_topic_info.get('description', '')
    }
    timeline_result = call_gpt_for_timeline(
        project_info=project_info_for_prompt,
        members_with_subtasks=members
    )

    # 3. éªŒè¯ç»“æœ
    print(f"  éªŒè¯æ—¶é—´çº¿åˆ†é…...")
    validation = validate_timeline(
        project_data=project_data,
        timeline_result=timeline_result,
        start_date=config.TIMELINE_START_DATE,
        end_date=config.TIMELINE_END_DATE
    )

    if not validation['valid']:
        print(f"  âŒ éªŒè¯å¤±è´¥:")
        for error in validation['errors']:
            print(f"    - {error}")
        raise ValueError(f"é¡¹ç›® {project_name} æ—¶é—´çº¿åˆ†é…éªŒè¯å¤±è´¥")

    if validation['warnings']:
        print(f"  âš ï¸  è­¦å‘Š:")
        for warning in validation['warnings']:
            print(f"    - {warning}")

    print(f"  âœ… éªŒè¯é€šè¿‡")

    # 4. åº”ç”¨æ—¶é—´çº¿åˆ°é¡¹ç›®æ•°æ®
    updated_project_data = apply_timeline_to_project(project_data, timeline_result)

    # 5. ä¿å­˜æ›´æ–°åçš„é¡¹ç›®æ–‡ä»¶
    save_project_file(str(project_path), updated_project_data)
    print(f"  ğŸ’¾ å·²ä¿å­˜æ›´æ–°åçš„é¡¹ç›®æ–‡ä»¶")

    # 6. è¿”å›å¤„ç†æŠ¥å‘Š
    return {
        'project_name': project_name,
        'sub_topic_id': sub_topic_info.get('sub_topic_id'),
        'parent_topic_id': sub_topic_info.get('parent_topic_id'),
        'success': True,
        'total_subtasks': total_subtasks,
        'assigned_tasks': validation['assigned_tasks'],
        'validation': validation,
        'timeline_summary': timeline_result.get('timeline_summary', {})
    }


def generate_summary_report(project_reports: list, output_dir: Path):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    report = {
        'generation_time': datetime.now().isoformat(),
        'total_projects': len(project_reports),
        'timeline_config': {
            'start_date': config.TIMELINE_START_DATE,
            'end_date': config.TIMELINE_END_DATE
        },
        'projects': project_reports,
        'statistics': {
            'total_tasks_processed': sum(p.get('total_subtasks', 0) for p in project_reports if p.get('success')),
            'successful_projects': len([p for p in project_reports if p.get('success')]),
            'failed_projects': len([p for p in project_reports if not p.get('success')])
        }
    }

    report_path = output_dir / 'timeline_assignment_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“Š æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    return report


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("Phase 5: ä»»åŠ¡æ—¶é—´çº¿åˆ†é…")
    print("="*60)

    # 1. ç¡®å®šé¡¹ç›®ç›®å½•
    output_dir = Path(config.OUTPUT_DIR)
    projects_dir = output_dir / 'projects'

    if not projects_dir.exists():
        print(f"âŒ é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: {projects_dir}")
        return

    # 2. æŸ¥æ‰¾æ‰€æœ‰é¡¹ç›®æ–‡ä»¶
    project_files = list(projects_dir.glob('*/*.json'))
    project_files = [f for f in project_files if f.stem != 'summary_report']

    if not project_files:
        print(f"âŒ æœªæ‰¾åˆ°é¡¹ç›®æ–‡ä»¶")
        return

    print(f"\næ‰¾åˆ° {len(project_files)} ä¸ªé¡¹ç›®")
    print(f"æ—¶é—´èŒƒå›´: {config.TIMELINE_START_DATE} ~ {config.TIMELINE_END_DATE}")
    print("")

    # 3. å¤„ç†æ¯ä¸ªé¡¹ç›®
    project_reports = []
    for project_file in tqdm(project_files, desc="å¤„ç†é¡¹ç›®"):
        try:
            report = process_single_project(project_file, output_dir)
            project_reports.append(report)
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {project_file.stem}")
            print(f"   é”™è¯¯: {e}")
            project_reports.append({
                'project_name': project_file.stem,
                'success': False,
                'error': str(e)
            })

    # 4. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_summary_report(project_reports, output_dir)

    # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    success_count = len([p for p in project_reports if p.get('success')])
    print(f"\n{'='*60}")
    print(f"âœ… æˆåŠŸ: {success_count}/{len(project_files)} ä¸ªé¡¹ç›®")
    if success_count < len(project_files):
        print(f"âŒ å¤±è´¥: {len(project_files) - success_count} ä¸ªé¡¹ç›®")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
