<div align="center">
  <h1>EverMemBench</h1>
  <p><strong>A comprehensive benchmark for quantifying and diagnosing memory systems in large language models</strong></p>
  <p>
    <a href="README.md">English</a> | <a href="README_zh.md">ÁÆÄ‰Ωì‰∏≠Êñá</a>
  </p>
</div>

## üìñ Project Description
EverMemBench is a benchmark designed to quantify and diagnose the memory systems of large language models. It introduces, for the first time, a three-tiered evaluation framework for memory systems consisting of: Factual Recall, Applied Memory, and Personalization Generalization.

This layered approach enables researchers to go beyond traditional retrieval-style evaluations and conduct fine-grained diagnostics of model capabilities, precisely locating performance bottlenecks in information extraction, contextual reasoning, or style adaptation. By offering a reproducible and standardized testing framework, EverMemBench not only reveals the significant shortcomings of current state-of-the-art models in achieving deep personalization, but also provides clear guidance for targeted optimization of memory systems.

## üåü Key Contributions
1. **Progressive memory evaluation framework**: We partition memory-system capabilities into three hierarchical layers ‚Äî **Factual Recall**, **Applied Memory**, and **Personalization Generalization** ‚Äî establishing a clear progression from pure retrieval to context integration to persona-consistent generation, thereby facilitating precise identification of performance bottlenecks.

2. **Realistic and diagnostic long-horizon multi-party chat dataset**: Grounded in real workplace communication scenarios, we construct a long-horizon corpus with a multi-role, multi-group, cross-context setting that explicitly models **temporal persona drift** and **community-switching effects**, enabling the assessment of memory robustness under concurrent topics and frequent context switches.

3. **Unified quantification and standardized evaluation protocol**: We provide consistent task formulations and measurement interfaces across the three core dimensions, supporting **reproducible** and **comparable** cross-model evaluation while reducing experimental bias in comparisons across systems and models.

4. **Systematic cross-model empirical analysis**: We comprehensively evaluate mainstream memory systems (e.g., MemOS, MemoryOS, Mem0, A-Mem) and state-of-the-art LLMs (e.g., GPT-4.5, GPT-4.1, Gemini-2.5-Pro), conducting **side-by-side comparisons** within a unified framework and revealing notable deficiencies in the memory capabilities of current advanced models.


## üóÇÔ∏è Benchmark Description
To systematically and reproducibly assess and diagnose LLM memory capabilities, we construct a long-horizon, multi-party group-chat dataset grounded in realistic workplace communication. The dataset centers on a ‚Äúmulti-role‚Äîmulti-group‚Äîcross-context‚Äù communication setting, explicitly modeling the dynamism and context-dependence of individual profiles. In real work scenarios, a person‚Äôs behavior and communicative style may drift over time as conversations unfold; at the same time, the same individual may act differently across communities/teams due to role relations and power structures. For example, a department director may be more decisive and stern within a direct-report team chat, yet more restrained in a cross-department strategic group among peers. We embed such ‚Äútime-varying‚Äù and ‚Äúcommunity-varying‚Äù personas and interaction patterns into the data construction process to faithfully reflect the complex and common communication ecology of enterprises.

Benefiting from this design, the dataset supports fine-grained and diagnostic evaluation of model memory systems under conditions of long conversations, concurrent topics, and frequent context switches. We summarize memory capability assessment along three core dimensions:

1. **Fine-grained Detailed Recall.** Tests retrieval ability, requiring the model to accurately reconstruct concrete facts from prior context.

2. **Memory Awareness.** Evaluates retrieval accompanied by understanding: the model must recall past events and integrate them to produce contextually appropriate answers.

3. **User Profile Understanding.** Focuses on personalization and adaptive generation. The model is expected to develop a stable understanding of individual preferences, roles, and tone based on historical interactions, and to adjust content and expression accordingly‚Äîavoiding replies that contradict the persona or are overly generic.


![‰∏ªÂõæ](./figures/main.png)


## üìä Benchmark Data
Coming Soon...


## üèóÔ∏è Benchmark Curation Pipeline
Coming Soon...


## üìà Performance on EverMemBench
Based on EverMemBench, we conducted a comprehensive evaluation of mainstream memory systems (e.g., MemOS, MemoryOS, Mem0, A-Mem) and state-of-the-art LLMs (e.g., GPT-4.5, GPT-4.1, Gemini-2.5-Pro), performing standardized measurements and cross-model comparisons across three core dimensions.


<!-- ## ÁõÆÂΩïÁªìÊûÑ

```
EverMemBench/
‚îú‚îÄ‚îÄ data/                    # Êï∞ÊçÆÊñá‰ª∂Â§π
‚îú‚îÄ‚îÄ figures/                 # ÂõæË°®Êñá‰ª∂Â§π
‚îú‚îÄ‚îÄ qa_annotation/           # ÈóÆÁ≠îÊ≥®ÈáäÊñá‰ª∂Â§π
‚îú‚îÄ‚îÄ scripts/                 # ËÑöÊú¨Êñá‰ª∂Â§π
‚îú‚îÄ‚îÄ api_tokens/              # API ÂØÜÈí•Êñá‰ª∂Â§πÔºàÈúÄÂàõÂª∫Ôºâ
‚îú‚îÄ‚îÄ .gitignore               # Git ÂøΩÁï•Êñá‰ª∂
‚îú‚îÄ‚îÄ LICENSE                  # ËÆ∏ÂèØËØÅÊñá‰ª∂
‚îú‚îÄ‚îÄ README.md                # È°πÁõÆËØ¥ÊòéÊñáÊ°£
‚îú‚îÄ‚îÄ config.yaml              # ÈÖçÁΩÆÊñá‰ª∂
‚îú‚îÄ‚îÄ conversation_infill.py   # ÂØπËØùÂ°´ÂÖÖËÑöÊú¨
‚îú‚îÄ‚îÄ inference.py             # Êé®ÁêÜËÑöÊú¨
‚îú‚îÄ‚îÄ inference_standalone_openai.py  # Áã¨Á´ãÁöÑ OpenAI Êé®ÁêÜËÑöÊú¨
‚îú‚îÄ‚îÄ prepare_blocks.py        # ÂáÜÂ§áÊï∞ÊçÆÂùóÁöÑËÑöÊú¨
‚îú‚îÄ‚îÄ prepare_data.py          # ÂáÜÂ§áÊï∞ÊçÆÁöÑËÑöÊú¨
‚îú‚îÄ‚îÄ prepare_qa.py            # ÂáÜÂ§áÈóÆÁ≠îÊï∞ÊçÆÁöÑËÑöÊú¨
‚îú‚îÄ‚îÄ prompts.py               # ÊèêÁ§∫ËØçËÑöÊú¨
‚îú‚îÄ‚îÄ query_llm.py             # Êü•ËØ¢Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÑöÊú¨
‚îî‚îÄ‚îÄ requirements.txt         # ‰æùËµñÈ°πÂàóË°®
``` -->

<!-- ## ÂÆâË£Ö

ÔºàÊ≠§Â§ÑÂ°´ÂÜôÂÆâË£ÖÊ≠•È™§Ôºâ -->

<!-- ## ‰ΩøÁî®ËØ¥Êòé

ÔºàÊ≠§Â§ÑÂ°´ÂÜô‰ΩøÁî®ËØ¥ÊòéÔºâ -->


<!-- ## Ë¥°ÁåÆ

ÔºàÊ≠§Â§ÑÂ°´ÂÜôË¥°ÁåÆÊåáÂçóÔºâ -->

## License

MIT license
